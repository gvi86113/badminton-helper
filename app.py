import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import re
from urllib.parse import urljoin

# --- è¨­å®šå°åŒ—æ™‚å€ ---
TP_TIMEZONE = pytz.timezone('Asia/Taipei')

# --- å·¥å…·å‡½å¼ ---
def get_current_time():
    # ç¢ºä¿å›å‚³ç•¶ä¸‹çš„å°åŒ—æ™‚é–“
    return datetime.now(TP_TIMEZONE)

def parse_taiwan_date(date_str):
    if not date_str:
        return None
    date_str = str(date_str).strip()
    
    # ã€ä¿®æ­£é‡é»ã€‘å„ªå…ˆå˜—è©¦åŒ¹é…è¥¿å…ƒå¹´ (4ç¢¼å¹´ä»½)ï¼Œé¿å… 2025 è¢«èª¤åˆ¤ç‚ºæ°‘åœ‹ 025 å¹´
    # æ ¼å¼: 2024-05-20, 2024/05/20
    western_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if western_match:
        year = int(western_match.group(1))
        month = int(western_match.group(2))
        day = int(western_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))

    # å†å˜—è©¦åŒ¹é…æ°‘åœ‹å¹´ (3ç¢¼å¹´ä»½)
    # æ ¼å¼: 113.05.20, 113-05-20, 113/05/20
    minguo_match = re.search(r'(\d{3})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if minguo_match:
        year = int(minguo_match.group(1)) + 1911
        month = int(minguo_match.group(2))
        day = int(minguo_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
        
    return None

# --- çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ ---
class SchoolScraper:
    def __init__(self, name, list_url, base_url, debug_mode=False):
        self.name = name
        self.list_url = list_url
        self.base_url = base_url
        self.debug = debug_mode
        self.logs = [] 

    def log(self, msg):
        if self.debug:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.logs.append(f"[{timestamp}] [{self.name}] {msg}")

    def fetch_data(self, days_limit=120):
        results = []
        try:
            self.log(f"é–‹å§‹è«‹æ±‚ç¶²å€: {self.list_url}")
            response = requests.get(self.list_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
            }, timeout=20)
            
            if response.status_code != 200:
                self.log(f"âŒ è«‹æ±‚å¤±æ•— (Status: {response.status_code})")
                return [], self.logs
            
            # è§£æé é¢
            if "syajh" in self.base_url:
                raw_items = self._parse_xingya(response.text)
            elif "nss" in self.list_url:
                raw_items = self._parse_nss(response.text)
            else:
                raw_items = []

            self.log(f"é é¢è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(raw_items)} å€‹æ½›åœ¨é …ç›®")
            
            # éæ¿¾è³‡æ–™
            filtered_results = []
            limit_date = get_current_time() - timedelta(days=days_limit)
            
            # å®šç¾©é—œéµå­— (OR é‚è¼¯)
            KEYWORDS = ["ç¾½çƒ", "å ´åœ°"]

            for item in raw_items:
                # 1. æ—¥æœŸæª¢æŸ¥
                item_date = parse_taiwan_date(item['date'])
                item['parsed_date'] = item_date
                
                short_title = (item['title'][:15] + '..') if len(item['title']) > 15 else item['title']
                debug_info = f"æ¨™é¡Œ: {short_title} | æ—¥æœŸ: {item['date']}"

                if not item_date:
                    self.log(f"âŒ æ—¥æœŸç„¡æ³•è§£æ: {debug_info}")
                    continue

                # 2. é—œéµå­—æª¢æŸ¥
                has_keyword = any(k in item['title'] for k in KEYWORDS)
                
                # 3. æ™‚é–“ç¯„åœèˆ‡éæ¿¾
                days_diff = (get_current_time() - item_date).days
                
                if item_date > limit_date:
                    if has_keyword:
                        filtered_results.append(item)
                        # åªæœ‰ç¬¦åˆé—œéµå­—çš„æ‰é¡¯ç¤ºç¶ è‰²å‹¾å‹¾ï¼Œä¿æŒç‰ˆé¢ä¹¾æ·¨
                        self.log(f"âœ… ä¿ç•™: {debug_info} (å‘½ä¸­é—œéµå­—)")
                    else:
                        # ä¸ç¬¦åˆé—œéµå­—çš„é …ç›®ï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸å¯«å…¥ Log å¹²æ“¾è¦–ç·šï¼Œé™¤éä½ éœ€è¦æ¥µåº¦è©³ç´°çš„é™¤éŒ¯
                        # self.log(f"âš ï¸ æ¨æ£„ (ç„¡é—œéµå­—): {debug_info}")
                        pass
                else:
                    # åªæœ‰ç•¶å®ƒã€Œæœ‰é—œéµå­—ã€ä½†ã€ŒéæœŸã€æ™‚æ‰é¡¯ç¤ºï¼Œé¿å…é¡¯ç¤ºä¸€å †éæœŸçš„ç„¡é—œå…¬å‘Š
                    if has_keyword:
                        self.log(f"â³ æ¨æ£„ (éæœŸ): {debug_info} (è·ä»Š {days_diff} å¤© > {days_limit} å¤©)")
            
            return filtered_results, self.logs
            
        except Exception as e:
            self.log(f"ğŸ”¥ ç¨‹å¼éŒ¯èª¤: {str(e)}")
            return [], self.logs

    def _parse_xingya(self, html):
        """
        èˆˆé›…åœ‹ä¸­è§£æå™¨ (åŠ å¼·ç‰ˆ)
        """
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        all_links = soup.find_all('a', href=True)
        
        self.log(f"æƒæé é¢ {len(all_links)} å€‹é€£çµ...")

        for link in all_links:
            title = link.get_text(strip=True)
            url = link['href']
            
            if len(title) < 4: continue # éæ¿¾ç„¡æ•ˆé€£çµ

            # å¾€ä¸Šæ‰¾çˆ¶å±¤æŠ“æ—¥æœŸ (å˜—è©¦ 4 å±¤ï¼Œç¢ºä¿æŠ“åˆ° RWD çš„ row)
            container = link
            found_date = None
            
            for _ in range(4): # å¾€ä¸Šçˆ¬ 4 å±¤
                if container.parent:
                    container = container.parent
                    row_text = container.get_text(" ", strip=True) # ç”¨ç©ºæ ¼åˆ†éš”
                    
                    # Regex: æŠ“ 2025-11-27
                    date_match = re.search(r'\d{4}-\d{2}-\d{2}', row_text)
                    if date_match:
                        found_date = date_match.group(0)
                        break # æ‰¾åˆ°äº†å°±åœæ­¢å¾€ä¸Šçˆ¬
                else:
                    break
            
            if found_date:
                full_url = urljoin(self.base_url, url)
                items.append({
                    "school": self.name,
                    "date": found_date,
                    "title": title,
                    "url": full_url
                })

        # å»é‡
        seen = set()
        unique_items = []
        for item in items:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_items.append(item)
        return unique_items

    def _parse_nss(self, html):
        """NSS ç³»çµ±è§£æå™¨"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        all_links = soup.find_all('a', href=True)
        
        for a_tag in all_links:
            container = a_tag
            found_date = None
            # åŒæ¨£å˜—è©¦å¾€ä¸Šçˆ¬
            for _ in range(3):
                if container.parent:
                    container = container.parent
                    row_text = container.get_text(" ", strip=True)
                    date_match = re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', row_text)
                    if date_match:
                        found_date = date_match.group(0)
                        break
            
            if found_date:
                title = a_tag.get_text(strip=True)
                if len(title) > 4:
                    items.append({
                        "school": self.name,
                        "date": found_date,
                        "title": title,
                        "url": urljoin(self.base_url, a_tag['href'])
                    })
        
        seen = set()
        unique = []
        for i in items:
            if i['url'] not in seen:
                seen.add(i['url'])
                unique.append(i)
        return unique

# --- Streamlit å‰ç«¯ ---

st.set_page_config(page_title="å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå…¬å‘Šå½™æ•´", layout="wide", page_icon="ğŸ¸")

st.sidebar.title("âš™ï¸ è¨­å®šèˆ‡é™¤éŒ¯")
debug_mode = st.sidebar.checkbox("é–‹å•Ÿå·¥ç¨‹å¸«é™¤éŒ¯æ¨¡å¼ (Show Logs)", value=True)
# é è¨­ 365 å¤©ï¼Œç¢ºä¿ä¸æœƒå› ç‚ºéæ¿¾å¤ªåš´æ ¼è€Œçœ‹èµ·ä¾†åƒæ²’è³‡æ–™
days_limit_input = st.sidebar.number_input("æœå°‹å¤©æ•¸ç¯„åœ (å¤©)", value=365, min_value=30, step=30) 

st.title("ğŸ¸ å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå ´åœ°å…¬å‘Š")
current_time = get_current_time()
st.caption(f"ç›®å‰ç³»çµ±æ™‚é–“ (å°åŒ—): {current_time.strftime('%Y-%m-%d %H:%M')}")

SCHOOL_LIST = [
    {"name": "èˆˆé›…åœ‹ä¸­", "base_url": "https://www.syajh.tp.edu.tw/", "list_url": "https://www.syajh.tp.edu.tw/more_infor.php?p_id=36"},
    # è¨»è§£å¦å¤–å…©é–“ï¼Œå°ˆæ³¨æ¸¬è©¦èˆˆé›…
    # {"name": "ä»æ„›åœ‹å°", "base_url": "https://www.japs.tp.edu.tw/", "list_url": "https://www.japs.tp.edu.tw/nss/main/freeze/5a9759adef37531ea27bf1b0/Cqfg8H21612"},
    # {"name": "ä¿¡ç¾©åœ‹å°", "base_url": "https://www.syes.tp.edu.tw/", "list_url": "https://www.syes.tp.edu.tw/nss/main/freeze/5abf2d62aa93092cee58ceb4/N84R5hZ3727"}
]

if st.button("ğŸ”„ ç«‹å³æ›´æ–°è³‡æ–™", type="primary"):
    st.cache_data.clear()
    st.rerun()

all_data = []
all_logs = {}

with st.spinner('æ­£åœ¨æƒæä¸¦éæ¿¾è³‡æ–™ (é—œéµå­—: ç¾½çƒ OR å ´åœ°)...'):
    for school in SCHOOL_LIST:
        scraper = SchoolScraper(school['name'], school['list_url'], school['base_url'], debug_mode=debug_mode)
        data, logs = scraper.fetch_data(days_limit=days_limit_input)
        all_data.extend(data)
        all_logs[school['name']] = logs

if not all_data:
    st.warning(f"è¿‘ {days_limit_input} å¤©å…§æ²’æœ‰æ‰¾åˆ°å«æœ‰ã€Œç¾½çƒã€æˆ–ã€Œå ´åœ°ã€çš„å…¬å‘Šã€‚")
else:
    df = pd.DataFrame(all_data)
    df = df.sort_values(by='parsed_date', ascending=False)
    
    st.success(f"å…±æ‰¾åˆ° {len(df)} ç­†å…¬å‘Š")
    
    for index, row in df.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 5])
            with col1:
                st.markdown(f"**{row['school']}**")
                st.caption(f"ğŸ“… {row['date']}")
            with col2:
                st.markdown(f"#### [{row['title']}]({row['url']})")
                if row['parsed_date']:
                    days_diff = (current_time - row['parsed_date']).days
                    if days_diff < 0:
                        st.caption(f"æœªä¾†å…¬å‘Š ({-days_diff} å¤©å¾Œ)")
                    else:
                        st.caption(f"{days_diff} å¤©å‰ç™¼å¸ƒ")

if debug_mode:
    st.markdown("---")
    st.subheader("ğŸ› ï¸ å·¥ç¨‹å¸«é™¤éŒ¯æ—¥èªŒ")
    for school_name, logs in all_logs.items():
        with st.expander(f"{school_name} - åŸ·è¡Œç´€éŒ„ ({len(logs)} è¡Œ)", expanded=True):
            for log in logs:
                if "âŒ" in log or "ğŸ”¥" in log:
                    st.error(log)
                elif "âš ï¸" in log:
                    st.warning(log)
                elif "âœ…" in log:
                    st.success(log)
                else:
                    st.text(log)
