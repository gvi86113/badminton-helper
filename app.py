import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import re
from urllib.parse import urljoin

# --- è¨­å®šå°åŒ—æ™‚å€ ---
TP_TIMEZONE = pytz.timezone('Asia/Taipei')

# --- å·¥å…·å‡½å¼ ---
def get_current_time():
    return datetime.now(TP_TIMEZONE)

def parse_taiwan_date(date_str):
    if not date_str:
        return None
    date_str = str(date_str).strip()
    
    # å˜—è©¦æŠ“å–å„ç¨®æ—¥æœŸæ ¼å¼
    # æ ¼å¼: 113.05.20, 113-05-20, 113/05/20
    minguo_match = re.search(r'(\d{3})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if minguo_match:
        year = int(minguo_match.group(1)) + 1911
        month = int(minguo_match.group(2))
        day = int(minguo_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
    
    # æ ¼å¼: 2024-05-20, 2024/05/20
    western_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if western_match:
        year = int(western_match.group(1))
        month = int(western_match.group(2))
        day = int(western_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
        
    return None

# --- çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ ---
class SchoolScraper:
    def __init__(self, name, list_url, base_url, debug_mode=False):
        self.name = name
        self.list_url = list_url
        self.base_url = base_url
        self.debug = debug_mode
        self.logs = [] # å„²å­˜ Log

    def log(self, msg):
        if self.debug:
            # åŠ ä¸Šæ™‚é–“æˆ³è¨˜æ–¹ä¾¿è¿½è¹¤
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.logs.append(f"[{timestamp}] [{self.name}] {msg}")

    def fetch_data(self, days_limit=120):
        results = []
        try:
            self.log(f"é–‹å§‹è«‹æ±‚ç¶²å€: {self.list_url}")
            # åŠ å…¥ Timeout é˜²æ­¢å¡ä½ï¼Œä¸¦æ¨¡æ“¬ç€è¦½å™¨ User-Agent
            response = requests.get(self.list_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
            }, timeout=20)
            
            self.log(f"HTTP ç‹€æ…‹ç¢¼: {response.status_code}")
            
            if response.status_code != 200:
                self.log("âŒ è«‹æ±‚å¤±æ•—ï¼Œè·³éæ­¤å­¸æ ¡")
                return [], self.logs
            
            # æ ¹æ“šå­¸æ ¡é¡å‹é¸æ“‡è§£æå™¨
            if "syajh" in self.base_url:
                raw_items = self._parse_xingya(response.text)
            elif "nss" in self.list_url:
                raw_items = self._parse_nss(response.text)
            else:
                raw_items = []

            self.log(f"é é¢è§£æå®Œæˆï¼Œå…±æŠ“åˆ° {len(raw_items)} å€‹æ½›åœ¨é …ç›® (æœªéæ¿¾)")
            
            # é–‹å§‹éæ¿¾è³‡æ–™
            filtered_results = []
            # è¨ˆç®—æˆªæ­¢æ—¥æœŸ (ä»Šå¤© - Nå¤©)
            limit_date = get_current_time() - timedelta(days=days_limit)
            
            for item in raw_items:
                # æ—¥æœŸè§£æ
                item_date = parse_taiwan_date(item['date'])
                item['parsed_date'] = item_date
                
                # ç°¡çŸ­æ¨™é¡Œç”¨æ–¼ Log
                short_title = (item['title'][:10] + '..') if len(item['title']) > 10 else item['title']
                debug_info = f"æ¨™é¡Œ: {short_title} | æ—¥æœŸ: {item['date']}"

                if not item_date:
                    self.log(f"âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤: {debug_info}")
                    continue

                # è¨ˆç®—é€™å‰‡å…¬å‘Šè·ä»Šå¹¾å¤©
                days_diff = (get_current_time() - item_date).days
                
                # é—œéµå­—æª¢æŸ¥ (å¯¬é¬†éæ¿¾)
                # åªè¦æ¨™é¡Œå«æœ‰ "ç¾½çƒ" æˆ– "ç§Ÿå€Ÿ" éƒ½å…ˆåˆ—å…¥è§€å¯Ÿ
                has_keyword = "ç¾½çƒ" in item['title']
                
                # åˆ¤æ–·æ˜¯å¦éæœŸ (æ³¨æ„ï¼šæœªä¾†çš„å…¬å‘Š item_date > limit_date æ†æˆç«‹ï¼Œæ‰€ä»¥æœªä¾†çš„ä¹Ÿæœƒè¢«æŠ“é€²ä¾†ï¼Œé€™æ˜¯æ­£ç¢ºçš„)
                if item_date > limit_date:
                    if has_keyword:
                        filtered_results.append(item)
                        self.log(f"âœ… ä¿ç•™: {debug_info}")
                    else:
                         self.log(f"âš ï¸ æ¨æ£„ (ç„¡é—œéµå­—): {debug_info}")
                else:
                    self.log(f"â³ æ¨æ£„ (éæœŸ): {debug_info} (è·ä»Š {days_diff} å¤©)")
            
            return filtered_results, self.logs
            
        except Exception as e:
            self.log(f"ğŸ”¥ ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")
            return [], self.logs

    def _parse_xingya(self, html):
        """
        èˆˆé›…åœ‹ä¸­è§£æå™¨ (é‡å° RWD æ”¹ç‰ˆå„ªåŒ–)
        ç­–ç•¥ï¼šä¸æ‰¾ Tableï¼Œç›´æ¥æ‰¾æ‰€æœ‰é€£çµï¼Œä¸¦å¾€çˆ¶å±¤æœå°‹æ—¥æœŸå­—ä¸²
        """
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        
        # 1. æŠ“å‡ºæ‰€æœ‰å¸¶æœ‰ href çš„é€£çµ
        all_links = soup.find_all('a', href=True)
        self.log(f"æƒæåˆ° {len(all_links)} å€‹é€£çµï¼Œé–‹å§‹åˆ†æçµæ§‹...")
        
        for link in all_links:
            title = link.get_text(strip=True)
            url = link['href']
            
            # éæ¿¾æ‰æ˜é¡¯ä¸æ˜¯å…¬å‘Šçš„é€£çµ (ä¾‹å¦‚ "å›é¦–é ", "æ›´å¤š", "å¦‚æœæ˜¯ç©ºå­—ä¸²")
            if len(title) < 4:
                continue
                
            # 2. å¾€ä¸Šæ‰¾çˆ¶å±¤å…ƒç´ ä¾†æŠ“æ—¥æœŸ
            # èˆˆé›…çš„æ—¥æœŸé€šå¸¸åœ¨é€£çµçš„æ—é‚Šï¼Œæˆ–æ˜¯ä¸Šä¸€å±¤ div è£¡
            try:
                # æŠ“å–è©²é€£çµæ‰€åœ¨çš„ã€Œå®¹å™¨ã€æ–‡å­—
                # parent æ˜¯ä¸Šä¸€å±¤ï¼Œparent.parent æ˜¯ä¸Šä¸Šä¸€å±¤ (é€šå¸¸èƒ½æ¶µè“‹æ•´è¡Œ)
                container = link.parent
                row_text = container.get_text()
                
                # å¦‚æœä¸Šä¸€å±¤æ–‡å­—å¤ªå°‘ï¼Œå¯èƒ½æ’ç‰ˆæ¯”è¼ƒæ·±ï¼Œå†å¾€ä¸Šä¸€å±¤æ‰¾
                if len(row_text) < 20 and link.parent.parent:
                     container = link.parent.parent
                     row_text = container.get_text()

                # ä½¿ç”¨ Regex æŠ“å–æ—¥æœŸ (æ ¼å¼: 2025-11-21 æˆ– 113-11-21)
                # é€™è£¡é‡å°èˆˆé›…æˆªåœ–çœ‹åˆ°çš„ 2025-11-21 åšå„ªåŒ–
                date_match = re.search(r'\d{4}-\d{2}-\d{2}', row_text)
                
                if date_match:
                    date_str = date_match.group(0)
                    full_url = urljoin(self.base_url, url)
                    
                    items.append({
                        "school": self.name,
                        "date": date_str,
                        "title": title,
                        "url": full_url
                    })
            except Exception:
                # çµæ§‹å¦‚æœä¸å°å°±è·³éï¼Œä¸å½±éŸ¿å…¶ä»–é€£çµ
                continue

        # å»é™¤é‡è¤‡ (RWD é é¢å¸¸æœƒæœ‰é›»è…¦ç‰ˆ/æ‰‹æ©Ÿç‰ˆå…©å€‹ä¸€æ¨£çš„é€£çµ)
        seen = set()
        unique_items = []
        for item in items:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_items.append(item)
                
        return unique_items

    def _parse_nss(self, html):
        """
        NSS ç³»çµ±è§£æå™¨ (ä»æ„›ã€ä¿¡ç¾©)
        """
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        all_links = soup.find_all('a', href=True)
        
        for a_tag in all_links:
            # å˜—è©¦å¾€ä¸Šæ‰¾æ—¥æœŸ
            container = a_tag.parent.parent if a_tag.parent else a_tag
            text_context = container.get_text()
            
            # æ”¯æ´ 2024/11/20 æˆ– 2024-11-20
            date_match = re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', text_context)
            
            if date_match:
                title = a_tag.get_text(strip=True)
                if len(title) > 4:
                    items.append({
                        "school": self.name,
                        "date": date_match.group(0),
                        "title": title,
                        "url": urljoin(self.base_url, a_tag['href'])
                    })
        
        seen = set()
        unique = []
        for i in items:
            if i['url'] not in seen:
                seen.add(i['url'])
                unique.append(i)
        return unique

# --- Streamlit å‰ç«¯ä»‹é¢ ---

st.set_page_config(page_title="å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå…¬å‘Šå½™æ•´", layout="wide", page_icon="ğŸ¸")

# å´é‚Šæ¬„è¨­å®š
st.sidebar.title("âš™ï¸ è¨­å®šèˆ‡é™¤éŒ¯")
debug_mode = st.sidebar.checkbox("é–‹å•Ÿå·¥ç¨‹å¸«é™¤éŒ¯æ¨¡å¼ (Show Logs)", value=True)
# é è¨­å¤©æ•¸è¨­ç‚º 400ï¼Œé¿å…å› ç‚ºç³»çµ±æ™‚é–“èˆ‡å…¬å‘Šæ™‚é–“è·¨å¹´å°è‡´çœ‹ä¸åˆ°
days_limit_input = st.sidebar.number_input("æœå°‹å¤©æ•¸ç¯„åœ (å¤©)", value=400, min_value=30, step=30) 

st.title("ğŸ¸ å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå ´åœ°å…¬å‘Š")
st.caption(f"ç›®å‰ç³»çµ±æ™‚é–“ (å°åŒ—): {get_current_time().strftime('%Y-%m-%d %H:%M')}")

# å®šç¾©å­¸æ ¡æ¸…å–®
# è¨»è§£æ‰æš«æ™‚æœ‰å•é¡Œçš„å­¸æ ¡ï¼Œå…ˆå°ˆæ³¨ä¿®å¾©èˆˆé›…
SCHOOL_LIST = [
    {
        "name": "èˆˆé›…åœ‹ä¸­", 
        "base_url": "https://www.syajh.tp.edu.tw/", 
        "list_url": "https://www.syajh.tp.edu.tw/more_infor.php?p_id=36"
    },
    # {
    #     "name": "ä»æ„›åœ‹å°", 
    #     "base_url": "https://www.japs.tp.edu.tw/", 
    #     "list_url": "https://www.japs.tp.edu.tw/nss/main/freeze/5a9759adef37531ea27bf1b0/Cqfg8H21612"
    # },
    # {
    #     "name": "ä¿¡ç¾©åœ‹å°", 
    #     "base_url": "https://www.syes.tp.edu.tw/", 
    #     "list_url": "https://www.syes.tp.edu.tw/nss/main/freeze/5abf2d62aa93092cee58ceb4/N84R5hZ3727"
    # }
]

if st.button("ğŸ”„ ç«‹å³æ›´æ–°è³‡æ–™", type="primary"):
    st.cache_data.clear()
    st.rerun()

all_data = []
all_logs = {}

# åŸ·è¡Œçˆ¬èŸ²
with st.spinner('æ©Ÿå™¨äººæ­£åœ¨æƒæå­¸æ ¡å®˜ç¶²...'):
    for school in SCHOOL_LIST:
        scraper = SchoolScraper(school['name'], school['list_url'], school['base_url'], debug_mode=debug_mode)
        data, logs = scraper.fetch_data(days_limit=days_limit_input)
        all_data.extend(data)
        all_logs[school['name']] = logs

# é¡¯ç¤ºçµæœå€
if not all_data:
    st.warning(f"è¿‘ {days_limit_input} å¤©å…§æ²’æœ‰æ‰¾åˆ°å«æœ‰ã€Œç¾½çƒã€é—œéµå­—çš„å…¬å‘Šã€‚è«‹æª¢æŸ¥é™¤éŒ¯æ—¥èªŒã€‚")
else:
    # è½‰æ›æˆ DataFrame æ–¹ä¾¿è™•ç†
    df = pd.DataFrame(all_data)
    # ä¾ç…§æ—¥æœŸæ’åº (æ–° -> èˆŠ)
    df = df.sort_values(by='parsed_date', ascending=False)
    
    st.success(f"å…±æ‰¾åˆ° {len(df)} ç­†å…¬å‘Š")
    
    # å¡ç‰‡å¼é¡¯ç¤º
    for index, row in df.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 5])
            with col1:
                st.markdown(f"**{row['school']}**")
                # é¡¯ç¤ºæ—¥æœŸ (å¦‚æœæœ‰ parsing æˆåŠŸ)
                date_display = row['date']
                st.caption(f"ğŸ“… {date_display}")
            with col2:
                st.markdown(f"#### [{row['title']}]({row['url']})")
                # å¯ä»¥åœ¨é€™è£¡åŠ å…¥æ›´å¤šè³‡è¨Šï¼Œä¾‹å¦‚ "è·ä»Š X å¤©"
                if row['parsed_date']:
                    days_ago = (get_current_time() - row['parsed_date']).days
                    if days_ago < 0:
                        st.caption(f"æœªä¾†å…¬å‘Š ({-days_ago} å¤©å¾Œ)")
                    else:
                        st.caption(f"{days_ago} å¤©å‰ç™¼å¸ƒ")

# é¡¯ç¤ºé™¤éŒ¯ Log
if debug_mode:
    st.markdown("---")
    st.subheader("ğŸ› ï¸ å·¥ç¨‹å¸«é™¤éŒ¯æ—¥èªŒ (Debug Logs)")
    for school_name, logs in all_logs.items():
        with st.expander(f"{school_name} - åŸ·è¡Œç´€éŒ„", expanded=True):
            for log in logs:
                if "âŒ" in log or "ğŸ”¥" in log:
                    st.error(log)
                elif "âš ï¸" in log or "â³" in log:
                    st.warning(log)
                elif "âœ…" in log:
                    st.success(log)
                else:
                    st.text(log)
