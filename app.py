import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import re
from urllib.parse import urljoin

# --- è¨­å®šå°åŒ—æ™‚å€ ---
TP_TIMEZONE = pytz.timezone('Asia/Taipei')

# --- å·¥å…·å‡½å¼ ---
def get_current_time():
    return datetime.now(TP_TIMEZONE)

def parse_taiwan_date(date_str):
    if not date_str:
        return None
    date_str = str(date_str).strip()
    
    # å˜—è©¦æŠ“å–å„ç¨®æ—¥æœŸæ ¼å¼
    # æ ¼å¼: 113.05.20, 113-05-20, 113/05/20
    minguo_match = re.search(r'(\d{3})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if minguo_match:
        year = int(minguo_match.group(1)) + 1911
        month = int(minguo_match.group(2))
        day = int(minguo_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
    
    # æ ¼å¼: 2024-05-20, 2024/05/20
    western_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if western_match:
        year = int(western_match.group(1))
        month = int(western_match.group(2))
        day = int(western_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
        
    return None

# --- çˆ¬èŸ²é‚è¼¯ ---
class SchoolScraper:
    def __init__(self, name, list_url, base_url, debug_mode=False):
        self.name = name
        self.list_url = list_url
        self.base_url = base_url
        self.debug = debug_mode
        self.logs = [] # å„²å­˜ Log

    def log(self, msg):
        if self.debug:
            self.logs.append(f"[{self.name}] {msg}")

    def fetch_data(self, days_limit=120):
        results = []
        try:
            self.log(f"é–‹å§‹è«‹æ±‚ç¶²å€: {self.list_url}")
            response = requests.get(self.list_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
            }, timeout=15)
            
            self.log(f"HTTP ç‹€æ…‹ç¢¼: {response.status_code}")
            
            if response.status_code != 200:
                self.log("è«‹æ±‚å¤±æ•—ï¼Œè·³é")
                return [], self.logs
            
            # è§£æ
            if "syajh" in self.base_url:
                raw_items = self._parse_xingya(response.text)
            elif "nss" in self.list_url:
                raw_items = self._parse_nss(response.text)
            else:
                raw_items = []

            self.log(f"åŸå§‹æŠ“å–ç­†æ•¸ (æœªéæ¿¾): {len(raw_items)}")
            
            # éæ¿¾
            filtered_results = []
            limit_date = get_current_time() - timedelta(days=days_limit)
            
            for item in raw_items:
                # æ—¥æœŸè§£æèˆ‡æª¢æŸ¥
                item_date = parse_taiwan_date(item['date'])
                item['parsed_date'] = item_date
                
                debug_info = f"æ¨™é¡Œ: {item['title'][:10]}... | æ—¥æœŸ: {item['date']}"

                if not item_date:
                    self.log(f"âŒ æ—¥æœŸè§£æå¤±æ•—: {debug_info}")
                    continue

                days_diff = (get_current_time() - item_date).days
                
                # é—œéµå­—éæ¿¾ (å¯¬é¬†ä¸€é»ï¼Œå…ˆä¸éæ¿¾ç¾½çƒï¼Œåªæ¨™è¨˜)
                has_keyword = "ç¾½çƒ" in item['title']
                
                if item_date > limit_date:
                    if has_keyword:
                        filtered_results.append(item)
                        self.log(f"âœ… ä¿ç•™: {debug_info} (è·ä»Š {days_diff} å¤©)")
                    else:
                         self.log(f"âš ï¸ æ¨æ£„ (ç„¡é—œéµå­—): {debug_info}")
                else:
                    self.log(f"â³ æ¨æ£„ (éæœŸ): {debug_info} (è·ä»Š {days_diff} å¤©, é™åˆ¶ {days_limit} å¤©)")
            
            return filtered_results, self.logs
            
        except Exception as e:
            self.log(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return [], self.logs

    def _parse_xingya(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        rows = soup.find_all('tr')
        self.log(f"æ‰¾åˆ° {len(rows)} å€‹è¡¨æ ¼åˆ—")
        for row in rows:
            text = row.get_text()
            # èˆˆé›…æ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ 2024-11-20
            date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
            a_tag = row.find('a')
            
            if date_match and a_tag:
                items.append({
                    "school": self.name,
                    "date": date_match.group(0),
                    "title": a_tag.get_text(strip=True),
                    "url": urljoin(self.base_url, a_tag['href'])
                })
        return items

    def _parse_nss(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        # NSS ç³»çµ±çµæ§‹è¤‡é›œï¼Œæ”¹ç”¨æ›´æš´åŠ›çš„æŠ“æ³•ï¼šæŠ“æ‰€æœ‰é€£çµï¼Œå¾€ä¸Šæ‰¾æ—¥æœŸ
        all_links = soup.find_all('a', href=True)
        self.log(f"æƒæé é¢é€£çµæ•¸: {len(all_links)}")
        
        for a_tag in all_links:
            # å¾€ä¸Šæ‰¾ 2 å±¤çˆ¶å…ƒç´ ä¾†æœå°‹æ—¥æœŸæ–‡å­—
            container = a_tag.parent.parent if a_tag.parent else a_tag
            text_context = container.get_text()
            
            # æ”¯æ´ 2024/11/20 æˆ– 2024-11-20
            date_match = re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', text_context)
            
            if date_match:
                title = a_tag.get_text(strip=True)
                # éæ¿¾æ‰å¤ªçŸ­çš„å°è¦½åˆ—é€£çµ
                if len(title) > 4:
                    items.append({
                        "school": self.name,
                        "date": date_match.group(0),
                        "title": title,
                        "url": urljoin(self.base_url, a_tag['href'])
                    })
        
        # å»é‡
        seen = set()
        unique = []
        for i in items:
            if i['url'] not in seen:
                seen.add(i['url'])
                unique.append(i)
        return unique

# --- ä¸»ç¨‹å¼ ---

st.set_page_config(page_title="å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå…¬å‘Šå½™æ•´", layout="wide", page_icon="ğŸ¸")

st.sidebar.title("âš™ï¸ è¨­å®šèˆ‡é™¤éŒ¯")
debug_mode = st.sidebar.checkbox("é–‹å•Ÿå·¥ç¨‹å¸«é™¤éŒ¯æ¨¡å¼ (Show Logs)", value=True)
days_limit_input = st.sidebar.number_input("æœå°‹å¤©æ•¸ç¯„åœ (å¤©)", value=365, min_value=30, max_value=9999) # é è¨­æ”¹å¤§ä¸€é»æ¸¬è©¦

st.title("ğŸ¸ å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå ´åœ°å…¬å‘Š")
st.caption(f"ç›®å‰ç³»çµ±æ™‚é–“: {get_current_time().strftime('%Y-%m-%d %H:%M')}")

SCHOOL_LIST = [
    {"name": "èˆˆé›…åœ‹ä¸­", "base_url": "https://www.syajh.tp.edu.tw/", "list_url": "https://www.syajh.tp.edu.tw/more_infor.php?p_id=36"},
    {"name": "ä»æ„›åœ‹å°", "base_url": "https://www.japs.tp.edu.tw/", "list_url": "https://www.japs.tp.edu.tw/nss/main/freeze/5a9759adef37531ea27bf1b0/Cqfg8H21612"},
    {"name": "ä¿¡ç¾©åœ‹å°", "base_url": "https://www.syes.tp.edu.tw/", "list_url": "https://www.syes.tp.edu.tw/nss/main/freeze/5abf2d62aa93092cee58ceb4/N84R5hZ3727"}
]

if st.button("ğŸ”„ ç«‹å³æ›´æ–°è³‡æ–™", type="primary"):
    st.cache_data.clear()
    st.rerun()

all_data = []
all_logs = {}

with st.spinner('æ©Ÿå™¨äººå·¡é‚ä¸­...'):
    for school in SCHOOL_LIST:
        scraper = SchoolScraper(school['name'], school['list_url'], school['base_url'], debug_mode=debug_mode)
        data, logs = scraper.fetch_data(days_limit=days_limit_input)
        all_data.extend(data)
        all_logs[school['name']] = logs

# é¡¯ç¤ºçµæœ
if not all_data:
    st.warning(f"è¿‘ {days_limit_input} å¤©å…§æ²’æœ‰æ‰¾åˆ°å«æœ‰ã€Œç¾½çƒã€é—œéµå­—çš„å…¬å‘Šã€‚")
else:
    df = pd.DataFrame(all_data)
    df = df.sort_values(by='parsed_date', ascending=False)
    st.success(f"å…±æ‰¾åˆ° {len(df)} ç­†å…¬å‘Š")
    
    for index, row in df.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 4])
            with col1:
                st.markdown(f"**{row['school']}**")
                st.caption(row['date'])
            with col2:
                st.markdown(f"[{row['title']}]({row['url']})")

# é¡¯ç¤ºé™¤éŒ¯ Log
if debug_mode:
    st.markdown("---")
    st.subheader("ğŸ› ï¸ å·¥ç¨‹å¸«é™¤éŒ¯æ—¥èªŒ (Debug Logs)")
    for school_name, logs in all_logs.items():
        with st.expander(f"{school_name} - åŸ·è¡Œç´€éŒ„", expanded=False):
            for log in logs:
                if "âŒ" in log or "âš ï¸" in log:
                    st.error(log)
                elif "âœ…" in log:
                    st.success(log)
                else:
                    st.text(log)
