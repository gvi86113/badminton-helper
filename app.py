import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import re
from urllib.parse import urljoin

# --- è¨­å®šå°åŒ—æ™‚å€ ---
TP_TIMEZONE = pytz.timezone('Asia/Taipei')

# --- å·¥å…·å‡½å¼ ---

def get_current_time():
    return datetime.now(TP_TIMEZONE)

def parse_taiwan_date(date_str):
    """
    å°‡å„ç¨®æ ¼å¼çš„æ—¥æœŸ (113-05-20, 2024/05/20) çµ±ä¸€è½‰ç‚º datetime ç‰©ä»¶
    """
    if not date_str:
        return None
    
    # ç§»é™¤ç©ºç™½èˆ‡ç‰¹æ®Šå­—å…ƒ
    date_str = date_str.strip()
    
    # è™•ç†æ°‘åœ‹å¹´ (ä¾‹å¦‚ 113-01-01 æˆ– 113/01/01)
    minguo_match = re.match(r'(\d{3})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if minguo_match:
        year = int(minguo_match.group(1)) + 1911
        month = int(minguo_match.group(2))
        day = int(minguo_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
    
    # è™•ç†è¥¿å…ƒå¹´ (ä¾‹å¦‚ 2024-01-01)
    western_match = re.match(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if western_match:
        year = int(western_match.group(1))
        month = int(western_match.group(2))
        day = int(western_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
        
    return None

# --- çˆ¬èŸ²é‚è¼¯ ---

class SchoolScraper:
    def __init__(self, name, list_url, base_url):
        self.name = name
        self.list_url = list_url
        self.base_url = base_url
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'
        }

    def fetch_data(self, days_limit=120):
        """æŠ“å–ä¸¦å›å‚³ç¬¦åˆæ¢ä»¶çš„è³‡æ–™åˆ—è¡¨"""
        results = []
        try:
            response = requests.get(self.list_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return []
            
            # æ ¹æ“šä¸åŒå­¸æ ¡é¡å‹å‘¼å«ä¸åŒçš„è§£æå‡½å¼
            if "syajh" in self.base_url:
                results = self._parse_xingya(response.text)
            elif "nss" in self.list_url: # ä»æ„›ã€ä¿¡ç¾©ç­‰ NSS ç³»çµ±
                results = self._parse_nss(response.text)
            
            # éæ¿¾è³‡æ–™ï¼š1. åŒ…å«ã€Œç¾½çƒã€ 2. æ™‚é–“åœ¨é™åˆ¶å¤©æ•¸å…§
            filtered_results = []
            limit_date = get_current_time() - timedelta(days=days_limit)
            
            for item in results:
                # é—œéµå­—éæ¿¾
                if "ç¾½çƒ" not in item['title']:
                    continue
                
                # æ—¥æœŸéæ¿¾
                item_date = parse_taiwan_date(item['date'])
                if item_date and item_date > limit_date:
                    item['parsed_date'] = item_date # å­˜èµ·ä¾†åšæ’åºç”¨
                    filtered_results.append(item)
            
            return filtered_results
            
        except Exception as e:
            print(f"Error scraping {self.name}: {e}")
            return []

    def _parse_xingya(self, html):
        """è§£æèˆˆé›…åœ‹ä¸­ (å‚³çµ± PHP è¡¨æ ¼)"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        # èˆˆé›…çš„åˆ—è¡¨é€šå¸¸åœ¨è¡¨æ ¼å…§ï¼Œé€™è£¡æŠ“å–æ‰€æœ‰å«æœ‰é€£çµçš„åˆ—
        # å°‹æ‰¾çµæ§‹ï¼šé€šå¸¸æ˜¯ tr -> td -> a
        rows = soup.find_all('tr')
        for row in rows:
            # å˜—è©¦æ‰¾æ—¥æœŸ (æ ¼å¼é€šå¸¸æ˜¯ YYYY-MM-DD)
            text_content = row.get_text()
            date_match = re.search(r'\d{4}-\d{2}-\d{2}', text_content)
            
            a_tag = row.find('a')
            if date_match and a_tag:
                date_str = date_match.group(0)
                title = a_tag.get_text(strip=True)
                link = urljoin(self.base_url, a_tag['href'])
                
                items.append({
                    "school": self.name,
                    "date": date_str,
                    "title": title,
                    "url": link
                })
        return items

    def _parse_nss(self, html):
        """è§£æ NSS ç³»çµ± (ä»æ„›ã€ä¿¡ç¾©)"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        
        # NSS ç³»çµ±çµæ§‹é€šå¸¸æ˜¯ div åˆ—è¡¨
        # å˜—è©¦æŠ“å–å¸¸è¦‹çš„åˆ—è¡¨ classï¼Œé€™è£¡é‡å°ä½ æä¾›çš„ç¶²å€çµæ§‹é€²è¡Œé€šç”¨è§£æ
        # é€™äº›ç¶²ç«™é€šå¸¸å°‡æ¨™é¡Œæ”¾åœ¨ title å±¬æ€§æˆ–ç‰¹å®šçš„ div ä¸­
        
        # ç­–ç•¥ï¼šæŠ“å–æ‰€æœ‰å«æœ‰ href çš„å€å¡Šï¼Œä¸¦è©¦åœ–å¾å€å¡Šæ–‡å­—ä¸­åˆ†é›¢æ—¥æœŸèˆ‡æ¨™é¡Œ
        # NSS çš„åˆ—è¡¨é …ç›®é€šå¸¸åŒ…åœ¨ r-ent æˆ–é¡ä¼¼çµæ§‹ï¼Œæˆ–ç›´æ¥æ‰¾ data-date
        
        # å˜—è©¦æ›´é€šç”¨çš„æŠ“æ³•ï¼šæŠ“å–æ‰€æœ‰ "panel-heading" æˆ–åˆ—è¡¨é …ç›®
        # ç”±æ–¼ NSS çµæ§‹è¤‡é›œï¼Œæˆ‘å€‘é€™è£¡ç”¨ä¸€å€‹ trick: æŠ“å–æ‰€æœ‰é€£çµï¼Œæª¢æŸ¥å…¶çˆ¶å…ƒç´ æ˜¯å¦æœ‰æ—¥æœŸ
        
        for a_tag in soup.find_all('a', href=True):
            parent_text = a_tag.parent.get_text() if a_tag.parent else ""
            grandparent_text = a_tag.parent.parent.get_text() if a_tag.parent and a_tag.parent.parent else ""
            
            # åˆä½µæ–‡å­—ä¾†æ‰¾æ—¥æœŸ
            full_text = (a_tag.get_text() + " " + parent_text + " " + grandparent_text).strip()
            
            # å°‹æ‰¾æ—¥æœŸ (NSS å¸¸è¦‹æ ¼å¼: 2024/11/27 æˆ– 2024-11-27)
            date_match = re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', full_text)
            
            if date_match:
                title = a_tag.get_text(strip=True)
                # æ’é™¤å¤ªçŸ­çš„é€£çµæ–‡å­— (ä¾‹å¦‚ "æ›´å¤š")
                if len(title) > 4: 
                    link = urljoin(self.base_url, a_tag['href'])
                    items.append({
                        "school": self.name,
                        "date": date_match.group(0),
                        "title": title,
                        "url": link
                    })
        
        # å»é™¤é‡è¤‡ (NSS æœ‰æ™‚æœƒæœ‰æ‰‹æ©Ÿç‰ˆ/é›»è…¦ç‰ˆé‡è¤‡é€£çµ)
        seen = set()
        unique_items = []
        for item in items:
            key = item['url']
            if key not in seen:
                seen.add(key)
                unique_items.append(item)
                
        return unique_items

# --- ä¸»ç¨‹å¼ ---

st.set_page_config(page_title="å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå…¬å‘Šå½™æ•´", layout="wide", page_icon="ğŸ¸")

st.title("ğŸ¸ å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå ´åœ°å…¬å‘Šå½™æ•´ (è¿‘120å¤©)")
st.caption(f"ç›®å‰æ™‚é–“ (å°åŒ—): {get_current_time().strftime('%Y-%m-%d %H:%M')}")

# å®šç¾©ç›®æ¨™å­¸æ ¡èˆ‡å…¶åˆ—è¡¨ç¶²å€ (é€™æ˜¯é—œéµï¼Œä¸èƒ½åªç”¨å…§æ–‡ç¶²å€)
# é€™è£¡æ ¹æ“šä½ æä¾›çš„å…§æ–‡ç¶²å€ï¼Œæ¨å°å‡ºåˆ—è¡¨ç¶²å€
SCHOOL_LIST = [
    {
        "name": "èˆˆé›…åœ‹ä¸­",
        "base_url": "https://www.syajh.tp.edu.tw/",
        "list_url": "https://www.syajh.tp.edu.tw/more_infor.php?p_id=36"
    },
    {
        "name": "ä»æ„›åœ‹å°",
        "base_url": "https://www.japs.tp.edu.tw/",
        "list_url": "https://www.japs.tp.edu.tw/nss/main/freeze/5a9759adef37531ea27bf1b0/Cqfg8H21612" # æ¨å°å‡ºçš„å…¬å‘Šåˆ—è¡¨
    },
    {
        "name": "ä¿¡ç¾©åœ‹å°",
        "base_url": "https://www.syes.tp.edu.tw/",
        "list_url": "https://www.syes.tp.edu.tw/nss/main/freeze/5abf2d62aa93092cee58ceb4/N84R5hZ3727" # æ¨å°å‡ºçš„å…¬å‘Šåˆ—è¡¨
    }
]

# å¿«å–å‡½å¼ (æ¯ 30 åˆ†é˜æ›´æ–°ä¸€æ¬¡)
@st.cache_data(ttl=1800, show_spinner=False)
def get_all_school_data():
    all_data = []
    
    # å»ºç«‹é€²åº¦æ¢
    progress_text = "æ­£åœ¨æƒæå„æ ¡å…¬å‘Š..."
    my_bar = st.progress(0, text=progress_text)
    
    total_schools = len(SCHOOL_LIST)
    for idx, school_info in enumerate(SCHOOL_LIST):
        scraper = SchoolScraper(school_info['name'], school_info['list_url'], school_info['base_url'])
        data = scraper.fetch_data(days_limit=120)
        all_data.extend(data)
        
        # æ›´æ–°é€²åº¦
        progress = (idx + 1) / total_schools
        my_bar.progress(progress, text=f"å·²æƒæ: {school_info['name']} (æ‰¾åˆ° {len(data)} ç­†)")
        
    my_bar.empty()
    return all_data

# åŸ·è¡ŒæŒ‰éˆ•
if st.button("ğŸ”„ ç«‹å³æ›´æ–°è³‡æ–™", type="primary"):
    st.cache_data.clear()
    st.rerun()

# ç²å–è³‡æ–™
with st.spinner('è³‡æ–™å½™æ•´ä¸­...'):
    raw_data = get_all_school_data()

if not raw_data:
    st.warning("è¿‘ 120 å¤©å…§æ²’æœ‰æ‰¾åˆ°å«æœ‰ã€Œç¾½çƒã€é—œéµå­—çš„å…¬å‘Šã€‚")
else:
    # è½‰æ›æˆ DataFrame ä»¥åˆ©æ’åºèˆ‡é¡¯ç¤º
    df = pd.DataFrame(raw_data)
    
    # ç¢ºä¿ä¾ç…§æ—¥æœŸæ’åº (æ–° -> èˆŠ)
    df = df.sort_values(by='parsed_date', ascending=False)
    
    # é‡æ•´é¡¯ç¤ºè³‡æ–™
    display_df = df[['date', 'school', 'title', 'url']].copy()
    display_df.columns = ['å…¬å‘Šæ—¥æœŸ', 'å­¸æ ¡', 'æ¨™é¡Œ', 'é€£çµ']
    
    # é¡¯ç¤ºçµ±è¨ˆ
    st.success(f"å…±æ‰¾åˆ° {len(df)} ç­†å…¬å‘Š")

    # å¡ç‰‡å¼é¡¯ç¤º (æ¯”è¡¨æ ¼åœ¨æ‰‹æ©Ÿä¸Šæ›´å¥½è®€)
    for index, row in display_df.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 4])
            with col1:
                st.markdown(f"**{row['å­¸æ ¡']}**")
                st.caption(f"ğŸ“… {row['å…¬å‘Šæ—¥æœŸ']}")
            with col2:
                st.markdown(f"[{row['æ¨™é¡Œ']}]({row['é€£çµ']})")

    # å¦‚æœéœ€è¦è¡¨æ ¼æ¨¡å¼ï¼Œå¯ä»¥è§£é–‹ä¸‹é¢é€™è¡Œ
    # st.dataframe(display_df, hide_index=True, use_container_width=True, column_config={"é€£çµ": st.column_config.LinkColumn()})
