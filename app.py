import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import pytz
import re
from urllib.parse import urljoin

# --- è¨­å®šå°åŒ—æ™‚å€ ---
TP_TIMEZONE = pytz.timezone('Asia/Taipei')

# --- å·¥å…·å‡½å¼ ---
def get_current_time():
    # ç¢ºä¿å›å‚³ç•¶ä¸‹çš„å°åŒ—æ™‚é–“
    return datetime.now(TP_TIMEZONE)

def parse_taiwan_date(date_str):
    if not date_str:
        return None
    date_str = str(date_str).strip()
    
    # å„ªå…ˆå˜—è©¦åŒ¹é…è¥¿å…ƒå¹´ (4ç¢¼å¹´ä»½)
    western_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if western_match:
        year = int(western_match.group(1))
        month = int(western_match.group(2))
        day = int(western_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))

    # å†å˜—è©¦åŒ¹é…æ°‘åœ‹å¹´ (3ç¢¼å¹´ä»½)
    minguo_match = re.search(r'(\d{3})[./-](\d{1,2})[./-](\d{1,2})', date_str)
    if minguo_match:
        year = int(minguo_match.group(1)) + 1911
        month = int(minguo_match.group(2))
        day = int(minguo_match.group(3))
        return TP_TIMEZONE.localize(datetime(year, month, day))
        
    return None

# --- çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ ---
class SchoolScraper:
    def __init__(self, name, list_url, base_url, debug_mode=False):
        self.name = name
        self.list_url = list_url
        self.base_url = base_url
        self.debug = debug_mode
        self.logs = [] 

    def log(self, msg):
        if self.debug:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.logs.append(f"[{timestamp}] [{self.name}] {msg}")

    def fetch_data(self, days_limit=120, max_pages=3):
        """
        æ”¯æ´ç¿»é çš„è³‡æ–™æŠ“å–
        max_pages: æœ€å¤§ç¿»é æ•¸ (é è¨­ 3 é )
        """
        all_results = []
        current_url = self.list_url
        page_num = 0
        
        try:
            # ç¿»é è¿´åœˆï¼šåªè¦æœ‰ç¶²å€ä¸”é‚„æ²’è¶…éé æ•¸ä¸Šé™ï¼Œå°±ç¹¼çºŒæŠ“
            while current_url and page_num < max_pages:
                page_num += 1
                self.log(f"ğŸ“„ æ­£åœ¨è®€å–ç¬¬ {page_num} é : {current_url}")
                
                response = requests.get(current_url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
                }, timeout=20)
                
                if response.status_code != 200:
                    self.log(f"âŒ ç¬¬ {page_num} é è«‹æ±‚å¤±æ•— (Status: {response.status_code})")
                    break # é€™ä¸€é å¤±æ•—å°±åœæ­¢ç¿»é 
                
                # è§£æé é¢ (ç¾åœ¨æœƒå›å‚³ next_url)
                next_url = None
                raw_items = []
                
                if "syajh" in self.base_url:
                    raw_items, next_url = self._parse_xingya(response.text)
                elif "nss" in self.list_url:
                    raw_items, next_url = self._parse_nss(response.text)
                else:
                    raw_items, next_url = [], None

                self.log(f"ç¬¬ {page_num} é è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(raw_items)} å€‹é …ç›®ã€‚ä¸‹ä¸€é é€£çµ: {'æœ‰' if next_url else 'ç„¡'}")
                
                # --- é–‹å§‹éæ¿¾é€™ä¸€é çš„è³‡æ–™ ---
                limit_date = get_current_time() - timedelta(days=days_limit)
                KEYWORDS = ["ç¾½çƒ", "å ´åœ°"]

                for item in raw_items:
                    item_date = parse_taiwan_date(item['date'])
                    item['parsed_date'] = item_date
                    
                    short_title = (item['title'][:15] + '..') if len(item['title']) > 15 else item['title']
                    debug_info = f"æ¨™é¡Œ: {short_title} | æ—¥æœŸ: {item['date']}"

                    if not item_date:
                        self.log(f"âŒ æ—¥æœŸç„¡æ³•è§£æ: {debug_info}")
                        continue

                    has_keyword = any(k in item['title'] for k in KEYWORDS)
                    
                    if item_date > limit_date:
                        if has_keyword:
                            all_results.append(item)
                            self.log(f"âœ… ä¿ç•™: {debug_info} (å‘½ä¸­é—œéµå­—)")
                    else:
                        if has_keyword:
                            self.log(f"â³ æ¨æ£„ (éæœŸ): {debug_info}")

                # è¨­å®šä¸‹ä¸€è¼ªçš„ç¶²å€
                current_url = next_url
                
                # å¦‚æœæ²’æœ‰ä¸‹ä¸€é ï¼Œå°±è·³å‡ºè¿´åœˆ
                if not current_url:
                    self.log("ğŸ å·²ç„¡ä¸‹ä¸€é ï¼Œåœæ­¢ç¿»é ã€‚")
                    break
            
            return all_results, self.logs
            
        except Exception as e:
            self.log(f"ğŸ”¥ ç¨‹å¼éŒ¯èª¤: {str(e)}")
            return [], self.logs

    def _parse_xingya(self, html):
        """
        èˆˆé›…åœ‹ä¸­è§£æå™¨ (æ”¯æ´ç¿»é èˆ‡ RWD)
        å›å‚³: (items, next_page_url)
        """
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        
        # 1. æŠ“å–å…§å®¹
        all_links = soup.find_all('a', href=True)
        # self.log(f"æƒæé é¢ {len(all_links)} å€‹é€£çµ...") # Logå¤ªå¤šå…ˆè¨»è§£

        for link in all_links:
            title = link.get_text(strip=True)
            url = link['href']
            
            if len(title) < 4: continue

            container = link
            found_date = None
            
            for _ in range(4):
                if container.parent:
                    container = container.parent
                    row_text = container.get_text(" ", strip=True)
                    date_match = re.search(r'\d{4}-\d{2}-\d{2}', row_text)
                    if date_match:
                        found_date = date_match.group(0)
                        break
                else:
                    break
            
            if found_date:
                full_url = urljoin(self.base_url, url)
                items.append({
                    "school": self.name,
                    "date": found_date,
                    "title": title,
                    "url": full_url
                })

        # å»é‡
        seen = set()
        unique_items = []
        for item in items:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_items.append(item)
        
        # 2. æŠ“å–ä¸‹ä¸€é é€£çµ (æ›´å¯¬é¬†çš„æœå°‹é‚è¼¯)
        next_url = None
        # ç›´æ¥éæ­·æ‰€æœ‰é€£çµï¼Œæª¢æŸ¥æ–‡å­—å…§å®¹æ˜¯å¦åŒ…å«ã€Œä¸‹ä¸€é ã€
        pagination_links = soup.find_all('a', href=True)
        for link in pagination_links:
            # å»é™¤ç©ºç™½å¾Œæª¢æŸ¥æ–‡å­—
            link_text = link.get_text(strip=True)
            if "ä¸‹ä¸€é " in link_text:
                href = link['href']
                # æ’é™¤ javascript void æˆ–ç©ºé€£çµ
                if "javascript" not in href.lower() and href != "#":
                    full_url = urljoin(self.base_url, href)
                    # åªæœ‰ç•¶ç¶²å€ä¸ä¸€æ¨£æ™‚æ‰è¦–ç‚ºä¸‹ä¸€é  (é¿å…åŸåœ°æ‰“è½‰)
                    if full_url != self.list_url:
                        next_url = full_url
                        self.log(f"ğŸ”— ç™¼ç¾ç¿»é é€£çµ: {next_url}")
                        break
        
        return unique_items, next_url

    def _parse_nss(self, html):
        """NSS ç³»çµ±è§£æå™¨"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        all_links = soup.find_all('a', href=True)
        
        for a_tag in all_links:
            container = a_tag
            found_date = None
            for _ in range(3):
                if container.parent:
                    container = container.parent
                    row_text = container.get_text(" ", strip=True)
                    date_match = re.search(r'\d{4}[-/]\d{2}[-/]\d{2}', row_text)
                    if date_match:
                        found_date = date_match.group(0)
                        break
            
            if found_date:
                title = a_tag.get_text(strip=True)
                if len(title) > 4:
                    items.append({
                        "school": self.name,
                        "date": found_date,
                        "title": title,
                        "url": urljoin(self.base_url, a_tag['href'])
                    })
        
        seen = set()
        unique = []
        for i in items:
            if i['url'] not in seen:
                seen.add(i['url'])
                unique.append(i)
        
        # NSS ç³»çµ±é€šå¸¸æ˜¯å‹•æ…‹è¼‰å…¥æˆ–å–®é é¡¯ç¤ºè¼ƒå¤šï¼Œæš«ä¸æ”¯æ´ç°¡å–®ç¿»é 
        return unique, None

# --- Streamlit å‰ç«¯ ---

st.set_page_config(page_title="å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå…¬å‘Šå½™æ•´", layout="wide", page_icon="ğŸ¸")

st.sidebar.title("âš™ï¸ è¨­å®šèˆ‡é™¤éŒ¯")
debug_mode = st.sidebar.checkbox("é–‹å•Ÿå·¥ç¨‹å¸«é™¤éŒ¯æ¨¡å¼ (Show Logs)", value=True)
days_limit_input = st.sidebar.number_input("æœå°‹å¤©æ•¸ç¯„åœ (å¤©)", value=365, min_value=30, step=30)
# æ–°å¢ç¿»é è¨­å®š
max_pages_input = st.sidebar.number_input("æœ€å¤§ç¿»é æ•¸", value=3, min_value=1, max_value=10, help="è¨­å®šæ¯å€‹å­¸æ ¡æœ€å¤šå¾€å¾Œçˆ¬å¹¾é ")

st.title("ğŸ¸ å°åŒ—å¸‚å­¸æ ¡ç¾½çƒå ´åœ°å…¬å‘Š")
current_time = get_current_time()
st.caption(f"ç›®å‰ç³»çµ±æ™‚é–“ (å°åŒ—): {current_time.strftime('%Y-%m-%d %H:%M')}")

SCHOOL_LIST = [
    {"name": "èˆˆé›…åœ‹ä¸­", "base_url": "https://www.syajh.tp.edu.tw/", "list_url": "https://www.syajh.tp.edu.tw/more_infor.php?p_id=36"},
    # {"name": "ä»æ„›åœ‹å°", "base_url": "https://www.japs.tp.edu.tw/", "list_url": "https://www.japs.tp.edu.tw/nss/main/freeze/5a9759adef37531ea27bf1b0/Cqfg8H21612"},
    # {"name": "ä¿¡ç¾©åœ‹å°", "base_url": "https://www.syes.tp.edu.tw/", "list_url": "https://www.syes.tp.edu.tw/nss/main/freeze/5abf2d62aa93092cee58ceb4/N84R5hZ3727"}
]

if st.button("ğŸ”„ ç«‹å³æ›´æ–°è³‡æ–™", type="primary"):
    st.cache_data.clear()
    st.rerun()

all_data = []
all_logs = {}

with st.spinner(f'æ­£åœ¨æƒæä¸¦ç¿»é  (æœ€å¤š {max_pages_input} é )...'):
    for school in SCHOOL_LIST:
        scraper = SchoolScraper(school['name'], school['list_url'], school['base_url'], debug_mode=debug_mode)
        # å‚³å…¥ max_pages åƒæ•¸
        data, logs = scraper.fetch_data(days_limit=days_limit_input, max_pages=max_pages_input)
        all_data.extend(data)
        all_logs[school['name']] = logs

if not all_data:
    st.warning(f"è¿‘ {days_limit_input} å¤©å…§æ²’æœ‰æ‰¾åˆ°å«æœ‰ã€Œç¾½çƒã€æˆ–ã€Œå ´åœ°ã€çš„å…¬å‘Šã€‚")
else:
    df = pd.DataFrame(all_data)
    df = df.sort_values(by='parsed_date', ascending=False)
    
    st.success(f"å…±æ‰¾åˆ° {len(df)} ç­†å…¬å‘Š")
    
    for index, row in df.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 5])
            with col1:
                st.markdown(f"**{row['school']}**")
                st.caption(f"ğŸ“… {row['date']}")
            with col2:
                st.markdown(f"#### [{row['title']}]({row['url']})")
                if row['parsed_date']:
                    days_diff = (current_time - row['parsed_date']).days
                    if days_diff < 0:
                        st.caption(f"æœªä¾†å…¬å‘Š ({-days_diff} å¤©å¾Œ)")
                    else:
                        st.caption(f"{days_diff} å¤©å‰ç™¼å¸ƒ")

if debug_mode:
    st.markdown("---")
    st.subheader("ğŸ› ï¸ å·¥ç¨‹å¸«é™¤éŒ¯æ—¥èªŒ")
    for school_name, logs in all_logs.items():
        with st.expander(f"{school_name} - åŸ·è¡Œç´€éŒ„ ({len(logs)} è¡Œ)", expanded=True):
            for log in logs:
                if "âŒ" in log or "ğŸ”¥" in log:
                    st.error(log)
                elif "âš ï¸" in log:
                    st.warning(log)
                elif "âœ…" in log:
                    st.success(log)
                else:
                    st.text(log)
